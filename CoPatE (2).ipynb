{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  1 02:55:20 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   57C    P0   289W / 300W |  30967MiB / 49140MiB |     30%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          On   | 00000000:23:00.0 Off |                  Off |\n",
      "|  0%   31C    P0    72W / 300W |  19734MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    782922      C                                   25061MiB |\n",
      "|    0   N/A  N/A    806734      C                                    1853MiB |\n",
      "|    0   N/A  N/A    861368      C                                    4051MiB |\n",
      "|    1   N/A  N/A    379430      C                                   12531MiB |\n",
      "|    1   N/A  N/A    806734      C                                    7201MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from torch.cuda.amp import autocast as autocast\n",
    "from torch.cuda.amp import GradScaler as GradScaler\n",
    "import os\n",
    "\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "EPOCHS = 1\n",
    "MAX_TOKEN_COUNT = 512\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.2 s, sys: 4min 14s, total: 4min 51s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "train_df=pd.read_feather(\"Hierarchical major.feather\")\n",
    "val_df=pd.read_feather(\"./autodl-nas/USPTO-2M_Validation.feather\")\n",
    "\n",
    "# LABEL_COLUMNS=train_df.columns[11:]\n",
    "# LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMNS_3 = train_df.columns[11:-139]\n",
    "LABEL_COLUMNS_2 =train_df.columns[-129:]\n",
    "LABEL_COLUMNS_1 = train_df.columns[-138:-129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pass pandas dataframe, and tokeizer along with the max token length[128 default]\n",
    "    \n",
    "    Example: \n",
    "    -------\n",
    "    train_dataset = ToxicCommentsDataset(\n",
    "      train_df,\n",
    "      tokenizer,\n",
    "      max_token_len=MAX_TOKEN_COUNT\n",
    "    )\n",
    "\n",
    "    sample_item = train_dataset[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: BertTokenizer,\n",
    "        max_token_len: int = 512,\n",
    "        test= False\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        self.valid = test\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        if not self.valid:\n",
    "            comment_text = data_row['major claim']\n",
    "            label_3 = data_row[LABEL_COLUMNS_3]\n",
    "            label_2 = data_row[LABEL_COLUMNS_2]\n",
    "            label_1 = data_row[LABEL_COLUMNS_1]\n",
    "            \n",
    "        else:\n",
    "            comment_text = \"\".join(data_row['claims'])\n",
    "            label_3 = data_row[LABEL_COLUMNS_3]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            max_length=self.max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=True, # [CLS] & [SEP]\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True, #attention_mask\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        if not self.valid:\n",
    "            return dict(\n",
    "            comment_text=comment_text,\n",
    "            input_ids = encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            label_1=torch.FloatTensor(label_1),\n",
    "            label_2=torch.FloatTensor(label_2),\n",
    "            label_3=torch.FloatTensor(label_3),\n",
    "                \n",
    "        )\n",
    "        else:\n",
    "            return dict(\n",
    "                comment_text=comment_text,\n",
    "                input_ids = encoding[\"input_ids\"].flatten(),\n",
    "                attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "                label_3=torch.FloatTensor(label_3)\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PatentDataset(\n",
    "  train_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "val_dataset = PatentDataset(\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT,\n",
    "  test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,drop_last = True,pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last = True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "Bert_model = BertModel.from_pretrained(BERT_MODEL_NAME,return_dict = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tags_df = pd.read_feather(\"Tags.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tags_datasets(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: BertTokenizer,\n",
    "        max_token_len: int = 512\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        labels = data_row.Label\n",
    "        tags = data_row.Tags\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tags,\n",
    "            max_length=self.max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return dict(\n",
    "            index=index,\n",
    "            label = labels,\n",
    "            input_ids=encoding['input_ids'].flatten(),\n",
    "            attention_mask = encoding['attention_mask'].flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tags_dataset = Tags_datasets(data = Tags_df,tokenizer = tokenizer, max_token_len = MAX_TOKEN_COUNT)\n",
    "Tags_dataloader = DataLoader(Tags_dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf6bbd2c56a4a2788d52d0eb57f70cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tags_embeddings = {}\n",
    "\n",
    "for step,batch in tqdm(enumerate(Tags_dataloader),total = len(Tags_dataloader)):\n",
    "    label = batch['label'][0]\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = Bert_model(input_ids,attention_mask = attention_mask)\n",
    "    tag_embeddings = output.pooler_output.detach().cpu()\n",
    "    tags_embeddings[label] = tag_embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_embedding = np.concatenate([tags_embeddings[key] for key in LABEL_COLUMNS_3],axis = 0)\n",
    "tags_embedding = torch.tensor(tags_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_embedding = tags_embedding.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive learning loss funcation\n",
    "\n",
    "class ConLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, temperature = 0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, embeddings, label_1,label_2,label_3):\n",
    "       \n",
    "        # embeddings (batch_size,bert_hidden_layer) , labels (batch_size,num_classes)\n",
    "        similarities = F.cosine_similarity(embeddings.unsqueeze(1),embeddings.unsqueeze(0),dim=2)/self.temperature\n",
    "#         print(similarities)\n",
    "        \n",
    "        # mask\n",
    "        logits_mask = ~torch.eye(BATCH_SIZE,dtype=torch.bool).to(device)\n",
    "        labels_mask = ~torch.eye(BATCH_SIZE,dtype=torch.bool).to(device)\n",
    "\n",
    "        exp_logits = torch.exp(similarities) * logits_mask\n",
    "        loss = torch.log(exp_logits.sum(1,keepdim=True)) - similarities\n",
    "        \n",
    "        # labels \n",
    "        weight_1 = torch.matmul(label_1,label_1.T)\n",
    "        weight_2 = torch.matmul(label_2,label_2.T)\n",
    "        weight_3 = torch.matmul(label_3,label_3.T)\n",
    "\n",
    "        weight_1 = weight_1 / torch.diag(weight_1)\n",
    "        weight_1 = weight_1 * labels_mask\n",
    "        weight_1 = torch.where(weight_1!=1,torch.zeros_like(weight_1).to(device),weight_1)\n",
    "        weight_1 = weight_1/torch.sum(weight_1)\n",
    "        \n",
    "        weight_2 = weight_2 / torch.diag(weight_2)\n",
    "        weight_2 = weight_2 * labels_mask\n",
    "        weight_2 = torch.where(weight_2!=1,torch.zeros_like(weight_2).to(device),weight_2)\n",
    "        weight_2 = weight_2/torch.sum(weight_2)\n",
    "        \n",
    "        weight_3 = weight_3 / torch.diag(weight_3)\n",
    "        weight_3 = weight_3 * labels_mask\n",
    "        weight_3 = torch.where(weight_3!=1,torch.zeros_like(weight_3).to(device),weight_3)\n",
    "        weight_3 = weight_3/torch.sum(weight_3)\n",
    "        \n",
    "        weight = weight_1*0.2 + weight_2*0.3 + weight_3*0.5\n",
    "        \n",
    "        loss = weight * loss\n",
    "        loss = torch.mean(loss)\n",
    "        return loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes: int,labels_embeddings: torch.Tensor, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True) #load the pretrained bert model\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes) # add a linear layer to the bert\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.Weight_1 = nn.Linear(n_classes,1)\n",
    "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.batch_labels_embeddings = labels_embeddings.expand(BATCH_SIZE,n_classes,self.bert.config.hidden_size)\n",
    "        self.batch_labels_embeddings = self.batch_labels_embeddings.transpose(1, 2) # batch_size * 768 * 664\n",
    "        self.batch_labels_embeddings.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        outputs = outputs.last_hidden_state\n",
    "        \n",
    "        # label-wise attention\n",
    "        att = torch.bmm(outputs,self.batch_labels_embeddings)\n",
    "        att = F.softmax(att,dim=1)\n",
    "        joint_labels_output = torch.bmm(outputs.transpose(1,2),att)\n",
    "        joint_labels_output = F.relu(self.Weight_1(joint_labels_output).squeeze(2))\n",
    "\n",
    "        return joint_labels_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes: int ):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(768,n_classes)\n",
    "        self.dropout = nn.Dropout(0.10)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self,embeddings,labels = None):\n",
    "        \n",
    "        output = self.classifier(self.dropout(embeddings))\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output,labels)\n",
    "            output = (loss,output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6248, 62484)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertNetwork(len(LABEL_COLUMNS_3),tags_embedding).to(device)\n",
    "Conloss =  ConLoss().to(device)\n",
    "classifier = Classifier(len(LABEL_COLUMNS_3)).to(device)\n",
    "N_EPOCHS = EPOCHS\n",
    "\n",
    "steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 10\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "optimizer = AdamW([\n",
    "                {'params': model.parameters()},\n",
    "                {'params': classifier.parameters()}],\n",
    "                lr=5e-5\n",
    ")\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate(mydataloader):\n",
    "\n",
    "    print(\"\\nEvaluating...\")\n",
    "    #t0 = time.time()\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in tqdm(enumerate(mydataloader),total=len(mydataloader),desc='Eval'):\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         label_1 = batch['label_1'].to(device)\n",
    "#         label_2 = batch['label_2'].to(device)\n",
    "        label_3 = batch['label_3'].to(device)\n",
    "        # deactivate autograd\n",
    "        with autocast():\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "#                 loss1 = Conloss(outputs,label_1=label_1,label_2=label_2,label_3=label_3)\n",
    "                loss2,outputs = classifier(outputs,label_3)\n",
    "                if step%1000 ==0:\n",
    "                    print(f\"loss contrastive :loss1  loss classifier: {loss2}\")\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "\n",
    "#                 loss = loss1 +loss2\n",
    "                loss = loss2\n",
    "                total_loss = total_loss + loss.float().item()\n",
    "\n",
    "                outputs = outputs.detach().float().cpu().numpy()\n",
    "                labels = label_3.detach().float().cpu().numpy()\n",
    "                total_preds.append(outputs)\n",
    "                total_labels.append(labels)\n",
    "\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(mydataloader)\n",
    "\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    total_labels = np.concatenate(total_labels, axis=0)\n",
    "    model.train()\n",
    "    print(f\"Evaluate loss {total_loss / len(mydataloader)}\")\n",
    "    return avg_loss, total_preds, total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    \n",
    "    now=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    best_valid_loss = float('inf')\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    avg_loss = 0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    # iterate over batches\n",
    "    for step,batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader),desc=\"Train\"):\n",
    "        \n",
    "        if step%10000 == 0 and step!=0:\n",
    "            valid_loss,_,_ = evaluate(val_dataloader)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f\"./model/CoPatE_lr_5e-5_{now}.pt\")\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        label_1 = batch['label_1'].to(device)\n",
    "        label_2 = batch['label_2'].to(device)\n",
    "        label_3 = batch['label_3'].to(device)  \n",
    "        if step > 0.8*len(train_dataloader):\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss1 = Conloss(outputs,label_1=label_1,label_2=label_2,label_3=label_3)\n",
    "                loss2,_ = classifier(outputs,label_3)\n",
    "                loss = loss1 + loss2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if step%100 == 0:\n",
    "                print(f\"STEP {step}: loss contrastive :{loss1}  loss classifier: {loss2}\")\n",
    "        else:\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss,_ = classifier(outputs,label_3)\n",
    "            optimizer.zero_grad()\n",
    "            if step%100 == 0:\n",
    "                print(f\"STEP {step}: loss classifier: {loss}\")\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        outputs=outputs.detach().float().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(outputs)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"{step}: {avg_loss}\")\n",
    "  \n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275cb4391aae4cb3a324558cc677ccc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/62484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss classifier: 0.006539450027048588\n",
      "STEP 100: loss classifier: 0.005286253523081541\n",
      "STEP 200: loss classifier: 0.00577233312651515\n",
      "STEP 300: loss classifier: 0.006785879377275705\n",
      "STEP 400: loss classifier: 0.006582504138350487\n",
      "STEP 500: loss classifier: 0.0052382079884409904\n",
      "STEP 600: loss classifier: 0.005633514374494553\n",
      "STEP 700: loss classifier: 0.00442507304251194\n",
      "STEP 800: loss classifier: 0.006413094699382782\n",
      "STEP 900: loss classifier: 0.005706325173377991\n",
      "STEP 1000: loss classifier: 0.005523086991161108\n",
      "STEP 1100: loss classifier: 0.00558877270668745\n",
      "STEP 1200: loss classifier: 0.005359854083508253\n",
      "STEP 1300: loss classifier: 0.0065163918770849705\n",
      "STEP 1400: loss classifier: 0.003570703323930502\n",
      "STEP 1500: loss classifier: 0.007230360526591539\n",
      "STEP 1600: loss classifier: 0.00527672516182065\n",
      "STEP 1700: loss classifier: 0.00831750500947237\n",
      "STEP 1800: loss classifier: 0.00614092405885458\n",
      "STEP 1900: loss classifier: 0.004562295041978359\n",
      "STEP 2000: loss classifier: 0.005175690166652203\n",
      "STEP 2100: loss classifier: 0.004328576382249594\n",
      "STEP 2200: loss classifier: 0.004973523318767548\n",
      "STEP 2300: loss classifier: 0.005874122492969036\n",
      "STEP 2400: loss classifier: 0.006074834614992142\n",
      "STEP 2500: loss classifier: 0.005165891721844673\n",
      "STEP 2600: loss classifier: 0.006347378250211477\n",
      "STEP 2700: loss classifier: 0.004414495546370745\n",
      "STEP 2800: loss classifier: 0.0064386771991848946\n",
      "STEP 2900: loss classifier: 0.006268524099141359\n",
      "STEP 3000: loss classifier: 0.004725339822471142\n",
      "STEP 3100: loss classifier: 0.007049886975437403\n",
      "STEP 3200: loss classifier: 0.00693482905626297\n",
      "STEP 3300: loss classifier: 0.004265408031642437\n",
      "STEP 3400: loss classifier: 0.005237014498561621\n",
      "STEP 3500: loss classifier: 0.004754994995892048\n",
      "STEP 3600: loss classifier: 0.005606966558843851\n",
      "STEP 3700: loss classifier: 0.004462642595171928\n",
      "STEP 3800: loss classifier: 0.005920844618231058\n",
      "STEP 3900: loss classifier: 0.005650793667882681\n",
      "STEP 4000: loss classifier: 0.005147247575223446\n",
      "STEP 4100: loss classifier: 0.004062294028699398\n",
      "STEP 4200: loss classifier: 0.004970716778188944\n",
      "STEP 4300: loss classifier: 0.0050108847208321095\n",
      "STEP 4400: loss classifier: 0.007277118042111397\n",
      "STEP 4500: loss classifier: 0.006438725627958775\n",
      "STEP 4600: loss classifier: 0.004815090913325548\n",
      "STEP 4700: loss classifier: 0.00442942650988698\n",
      "STEP 4800: loss classifier: 0.00474846176803112\n",
      "STEP 4900: loss classifier: 0.005198792088776827\n",
      "STEP 5000: loss classifier: 0.005849637556821108\n",
      "STEP 5100: loss classifier: 0.004946219269186258\n",
      "STEP 5200: loss classifier: 0.0060136509127914906\n",
      "STEP 5300: loss classifier: 0.0045106373727321625\n",
      "STEP 5400: loss classifier: 0.004464352037757635\n",
      "STEP 5500: loss classifier: 0.006076584104448557\n",
      "STEP 5600: loss classifier: 0.005465583875775337\n",
      "STEP 5700: loss classifier: 0.00516209751367569\n",
      "STEP 5800: loss classifier: 0.0053649479523301125\n",
      "STEP 5900: loss classifier: 0.007043303456157446\n",
      "STEP 6000: loss classifier: 0.003617412643507123\n",
      "STEP 6100: loss classifier: 0.006839101202785969\n",
      "STEP 6200: loss classifier: 0.004684781655669212\n",
      "STEP 6300: loss classifier: 0.0053743477910757065\n",
      "STEP 6400: loss classifier: 0.004571681376546621\n",
      "STEP 6500: loss classifier: 0.004600910935550928\n",
      "STEP 6600: loss classifier: 0.006448788568377495\n",
      "STEP 6700: loss classifier: 0.005160000640898943\n",
      "STEP 6800: loss classifier: 0.0034001213498413563\n",
      "STEP 6900: loss classifier: 0.007344665005803108\n",
      "STEP 7000: loss classifier: 0.005656437948346138\n",
      "STEP 7100: loss classifier: 0.006074117962270975\n",
      "STEP 7200: loss classifier: 0.006808861158788204\n",
      "STEP 7300: loss classifier: 0.005906104110181332\n",
      "STEP 7400: loss classifier: 0.004647815600037575\n",
      "STEP 7500: loss classifier: 0.003704444970935583\n",
      "STEP 7600: loss classifier: 0.003730964846909046\n",
      "STEP 7700: loss classifier: 0.005923544522374868\n",
      "STEP 7800: loss classifier: 0.005582320503890514\n",
      "STEP 7900: loss classifier: 0.005360080860555172\n",
      "STEP 8000: loss classifier: 0.004627963528037071\n",
      "STEP 8100: loss classifier: 0.007505163084715605\n",
      "STEP 8200: loss classifier: 0.003970326390117407\n",
      "STEP 8300: loss classifier: 0.004550638608634472\n",
      "STEP 8400: loss classifier: 0.006320187821984291\n",
      "STEP 8500: loss classifier: 0.00592668866738677\n",
      "STEP 8600: loss classifier: 0.005469313357025385\n",
      "STEP 8700: loss classifier: 0.0039873141795396805\n",
      "STEP 8800: loss classifier: 0.006450480315834284\n",
      "STEP 8900: loss classifier: 0.005404149182140827\n",
      "STEP 9000: loss classifier: 0.004969788249582052\n",
      "STEP 9100: loss classifier: 0.006224627606570721\n",
      "STEP 9200: loss classifier: 0.004452972207218409\n",
      "STEP 9300: loss classifier: 0.004743544850498438\n",
      "STEP 9400: loss classifier: 0.005608418490737677\n",
      "STEP 9500: loss classifier: 0.006287720985710621\n",
      "STEP 9600: loss classifier: 0.0059137484058737755\n",
      "STEP 9700: loss classifier: 0.004379655234515667\n",
      "STEP 9800: loss classifier: 0.005186283495277166\n",
      "STEP 9900: loss classifier: 0.003130205674096942\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb40b96cfa2467a9b3e162b5b6a6b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/1634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :loss1  loss classifier: 0.005678244866430759\n",
      "loss contrastive :loss1  loss classifier: 0.004505077842622995\n",
      "Evaluate loss 0.005179616474293428\n",
      "STEP 10000: loss classifier: 0.003268686356022954\n",
      "STEP 10100: loss classifier: 0.0038966115098446608\n",
      "STEP 10200: loss classifier: 0.004677766002714634\n",
      "STEP 10300: loss classifier: 0.003893543966114521\n",
      "STEP 10400: loss classifier: 0.005082374904304743\n",
      "STEP 10500: loss classifier: 0.006504149176180363\n",
      "STEP 10600: loss classifier: 0.004028094932436943\n",
      "STEP 10700: loss classifier: 0.005414608400315046\n",
      "STEP 10800: loss classifier: 0.005910307168960571\n",
      "STEP 10900: loss classifier: 0.0037751924246549606\n",
      "STEP 11000: loss classifier: 0.005412472411990166\n",
      "STEP 11100: loss classifier: 0.004746235907077789\n",
      "STEP 11200: loss classifier: 0.004026717506349087\n",
      "STEP 11300: loss classifier: 0.00552744185552001\n",
      "STEP 11400: loss classifier: 0.005490193609148264\n",
      "STEP 11500: loss classifier: 0.005467909388244152\n",
      "STEP 11600: loss classifier: 0.004169811494648457\n",
      "STEP 11700: loss classifier: 0.006914353463798761\n",
      "STEP 11800: loss classifier: 0.005483405664563179\n",
      "STEP 11900: loss classifier: 0.004582169931381941\n",
      "STEP 12000: loss classifier: 0.0038280775770545006\n",
      "STEP 12100: loss classifier: 0.003453893354162574\n",
      "STEP 12200: loss classifier: 0.0032416675239801407\n",
      "STEP 12300: loss classifier: 0.005106169730424881\n",
      "STEP 12400: loss classifier: 0.0035235772375017405\n",
      "STEP 12500: loss classifier: 0.00561627559363842\n",
      "STEP 12600: loss classifier: 0.005738398991525173\n",
      "STEP 12700: loss classifier: 0.003878540126606822\n",
      "STEP 12800: loss classifier: 0.004083945881575346\n",
      "STEP 12900: loss classifier: 0.004049836192280054\n",
      "STEP 13000: loss classifier: 0.00510528776794672\n",
      "STEP 13100: loss classifier: 0.006889869458973408\n",
      "STEP 13200: loss classifier: 0.004974223207682371\n",
      "STEP 13300: loss classifier: 0.004655224736779928\n",
      "STEP 13400: loss classifier: 0.004989681299775839\n",
      "STEP 13500: loss classifier: 0.005113368853926659\n",
      "STEP 13600: loss classifier: 0.005444637965410948\n",
      "STEP 13700: loss classifier: 0.0054906015284359455\n",
      "STEP 13800: loss classifier: 0.005651654675602913\n",
      "STEP 13900: loss classifier: 0.004732327535748482\n",
      "STEP 14000: loss classifier: 0.004026091191917658\n",
      "STEP 14100: loss classifier: 0.004848966374993324\n",
      "STEP 14200: loss classifier: 0.005857745185494423\n",
      "STEP 14300: loss classifier: 0.003427040297538042\n",
      "STEP 14400: loss classifier: 0.00439917016774416\n",
      "STEP 14500: loss classifier: 0.003745216876268387\n",
      "STEP 14600: loss classifier: 0.005445610266178846\n",
      "STEP 14700: loss classifier: 0.004421268589794636\n",
      "STEP 14800: loss classifier: 0.00370035064406693\n",
      "STEP 14900: loss classifier: 0.004925579763948917\n",
      "STEP 15000: loss classifier: 0.0052367146126925945\n",
      "STEP 15100: loss classifier: 0.00462749321013689\n",
      "STEP 15200: loss classifier: 0.004251559730619192\n",
      "STEP 15300: loss classifier: 0.004231981933116913\n",
      "STEP 15400: loss classifier: 0.006167885381728411\n",
      "STEP 15500: loss classifier: 0.004680795595049858\n",
      "STEP 15600: loss classifier: 0.0061109126545488834\n",
      "STEP 15700: loss classifier: 0.005531971342861652\n",
      "STEP 15800: loss classifier: 0.003876479109749198\n",
      "STEP 15900: loss classifier: 0.004435302224010229\n",
      "STEP 16000: loss classifier: 0.00429546320810914\n",
      "STEP 16100: loss classifier: 0.004747147671878338\n",
      "STEP 16200: loss classifier: 0.0032267202623188496\n",
      "STEP 16300: loss classifier: 0.004396313801407814\n",
      "STEP 16400: loss classifier: 0.006031619384884834\n",
      "STEP 16500: loss classifier: 0.004430909641087055\n",
      "STEP 16600: loss classifier: 0.0044727446511387825\n",
      "STEP 16700: loss classifier: 0.003072087885811925\n",
      "STEP 16800: loss classifier: 0.0049435729160904884\n",
      "STEP 16900: loss classifier: 0.00625812029466033\n",
      "STEP 17000: loss classifier: 0.004164530895650387\n",
      "STEP 17100: loss classifier: 0.00490039587020874\n",
      "STEP 17200: loss classifier: 0.004136967938393354\n",
      "STEP 17300: loss classifier: 0.0064944373443722725\n",
      "STEP 17400: loss classifier: 0.004682906437665224\n",
      "STEP 17500: loss classifier: 0.005466298665851355\n",
      "STEP 17600: loss classifier: 0.003498862963169813\n",
      "STEP 17700: loss classifier: 0.006092342082411051\n",
      "STEP 17800: loss classifier: 0.004946501459926367\n",
      "STEP 17900: loss classifier: 0.0041702184826135635\n",
      "STEP 18000: loss classifier: 0.00591060658916831\n",
      "STEP 18100: loss classifier: 0.004028259310871363\n",
      "STEP 18200: loss classifier: 0.005959364585578442\n",
      "STEP 18300: loss classifier: 0.0042628017254173756\n",
      "STEP 18400: loss classifier: 0.0038272924721240997\n",
      "STEP 18500: loss classifier: 0.0048021250404417515\n",
      "STEP 18600: loss classifier: 0.005067909136414528\n",
      "STEP 18700: loss classifier: 0.0045791021548211575\n",
      "STEP 18800: loss classifier: 0.0031887213699519634\n",
      "STEP 18900: loss classifier: 0.0032324977219104767\n",
      "STEP 19000: loss classifier: 0.003831632435321808\n",
      "STEP 19100: loss classifier: 0.005328596103936434\n",
      "STEP 19200: loss classifier: 0.00544862262904644\n",
      "STEP 19300: loss classifier: 0.003920784220099449\n",
      "STEP 19400: loss classifier: 0.0037143512163311243\n",
      "STEP 19500: loss classifier: 0.004596956539899111\n",
      "STEP 19600: loss classifier: 0.00617477810010314\n",
      "STEP 19700: loss classifier: 0.005155806429684162\n",
      "STEP 19800: loss classifier: 0.004806154407560825\n",
      "STEP 19900: loss classifier: 0.004553326405584812\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb1c12755734d4ca3e62d29647c609f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/1634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :loss1  loss classifier: 0.005192983895540237\n",
      "loss contrastive :loss1  loss classifier: 0.0043900529853999615\n",
      "Evaluate loss 0.004882681305916932\n",
      "STEP 20000: loss classifier: 0.0031283122953027487\n",
      "STEP 20100: loss classifier: 0.0062094018794596195\n",
      "STEP 20200: loss classifier: 0.005252518225461245\n",
      "STEP 20300: loss classifier: 0.004182801581919193\n",
      "STEP 20400: loss classifier: 0.006056245416402817\n",
      "STEP 20500: loss classifier: 0.004632787313312292\n",
      "STEP 20600: loss classifier: 0.004803474526852369\n",
      "STEP 20700: loss classifier: 0.004556267522275448\n",
      "STEP 20800: loss classifier: 0.004039531573653221\n",
      "STEP 20900: loss classifier: 0.0046360306441783905\n",
      "STEP 21000: loss classifier: 0.0033480986021459103\n",
      "STEP 21100: loss classifier: 0.0044764308258891106\n",
      "STEP 21200: loss classifier: 0.0042644077911973\n",
      "STEP 21300: loss classifier: 0.004757292568683624\n",
      "STEP 21400: loss classifier: 0.004079400096088648\n",
      "STEP 21500: loss classifier: 0.004243966657668352\n",
      "STEP 21600: loss classifier: 0.0051841107197105885\n",
      "STEP 21700: loss classifier: 0.003999486565589905\n",
      "STEP 21800: loss classifier: 0.004800343420356512\n",
      "STEP 21900: loss classifier: 0.0034183887764811516\n",
      "STEP 22000: loss classifier: 0.0037320987321436405\n",
      "STEP 22100: loss classifier: 0.003605759935453534\n",
      "STEP 22200: loss classifier: 0.004718656651675701\n",
      "STEP 22300: loss classifier: 0.0036060726270079613\n",
      "STEP 22400: loss classifier: 0.006691909395158291\n",
      "STEP 22500: loss classifier: 0.0039035503286868334\n",
      "STEP 22600: loss classifier: 0.005770361516624689\n",
      "STEP 22700: loss classifier: 0.005181009881198406\n",
      "STEP 22800: loss classifier: 0.005599325988441706\n",
      "STEP 22900: loss classifier: 0.005007367115467787\n",
      "STEP 23000: loss classifier: 0.0038173117209225893\n",
      "STEP 23100: loss classifier: 0.004175203386694193\n",
      "STEP 23200: loss classifier: 0.0032995499204844236\n",
      "STEP 23300: loss classifier: 0.006531176622956991\n",
      "STEP 23400: loss classifier: 0.0026647939812391996\n",
      "STEP 23500: loss classifier: 0.004684073384851217\n",
      "STEP 23600: loss classifier: 0.0052034491673111916\n",
      "STEP 23700: loss classifier: 0.0038820309564471245\n",
      "STEP 23800: loss classifier: 0.005303988698869944\n",
      "STEP 23900: loss classifier: 0.005913442000746727\n",
      "STEP 24000: loss classifier: 0.004170616623014212\n",
      "STEP 24100: loss classifier: 0.004910001531243324\n",
      "STEP 24200: loss classifier: 0.005289814434945583\n",
      "STEP 24300: loss classifier: 0.004002076108008623\n",
      "STEP 24400: loss classifier: 0.0048272861167788506\n",
      "STEP 24500: loss classifier: 0.003768939757719636\n",
      "STEP 24600: loss classifier: 0.005686006974428892\n",
      "STEP 24700: loss classifier: 0.0036138445138931274\n",
      "STEP 24800: loss classifier: 0.004803616553544998\n",
      "STEP 24900: loss classifier: 0.004019830375909805\n",
      "STEP 25000: loss classifier: 0.005481757689267397\n",
      "STEP 25100: loss classifier: 0.0034742814023047686\n",
      "STEP 25200: loss classifier: 0.004294014070183039\n",
      "STEP 25300: loss classifier: 0.005713063292205334\n",
      "STEP 25400: loss classifier: 0.003961819689720869\n",
      "STEP 25500: loss classifier: 0.005820206832140684\n",
      "STEP 25600: loss classifier: 0.004417858086526394\n",
      "STEP 25700: loss classifier: 0.0045083435252308846\n",
      "STEP 25800: loss classifier: 0.005173299461603165\n",
      "STEP 25900: loss classifier: 0.007341837976127863\n",
      "STEP 26000: loss classifier: 0.006515072658658028\n",
      "STEP 26100: loss classifier: 0.006313210818916559\n",
      "STEP 26200: loss classifier: 0.004093687050044537\n",
      "STEP 26300: loss classifier: 0.004453397821635008\n",
      "STEP 26400: loss classifier: 0.005020456854254007\n",
      "STEP 26500: loss classifier: 0.0029351655393838882\n",
      "STEP 26600: loss classifier: 0.0032718933653086424\n",
      "STEP 26700: loss classifier: 0.003566149389371276\n",
      "STEP 26800: loss classifier: 0.005358760245144367\n",
      "STEP 26900: loss classifier: 0.003494714153930545\n",
      "STEP 27000: loss classifier: 0.005667104385793209\n",
      "STEP 27100: loss classifier: 0.005773637909442186\n",
      "STEP 27200: loss classifier: 0.0034466376528143883\n",
      "STEP 27300: loss classifier: 0.004065452143549919\n",
      "STEP 27400: loss classifier: 0.005535781849175692\n",
      "STEP 27500: loss classifier: 0.0050195734947919846\n",
      "STEP 27600: loss classifier: 0.0038595644291490316\n",
      "STEP 27700: loss classifier: 0.004676543641835451\n",
      "STEP 27800: loss classifier: 0.005052830092608929\n",
      "STEP 27900: loss classifier: 0.005169201642274857\n",
      "STEP 28000: loss classifier: 0.005618671886622906\n",
      "STEP 28100: loss classifier: 0.006025626324117184\n",
      "STEP 28200: loss classifier: 0.0050669326446950436\n",
      "STEP 28300: loss classifier: 0.006466419901698828\n",
      "STEP 28400: loss classifier: 0.004735690541565418\n",
      "STEP 28500: loss classifier: 0.0052511063404381275\n",
      "STEP 28600: loss classifier: 0.0038249753415584564\n",
      "STEP 28700: loss classifier: 0.006495116278529167\n",
      "STEP 28800: loss classifier: 0.005678198300302029\n",
      "STEP 28900: loss classifier: 0.005347545724362135\n",
      "STEP 29000: loss classifier: 0.005053682252764702\n",
      "STEP 29100: loss classifier: 0.00427790405228734\n",
      "STEP 29200: loss classifier: 0.0043044122867286205\n",
      "STEP 29300: loss classifier: 0.004247817676514387\n",
      "STEP 29400: loss classifier: 0.005226552952080965\n",
      "STEP 29500: loss classifier: 0.004059361293911934\n",
      "STEP 29600: loss classifier: 0.0031901467591524124\n",
      "STEP 29700: loss classifier: 0.0038596910890191793\n",
      "STEP 29800: loss classifier: 0.005160023458302021\n",
      "STEP 29900: loss classifier: 0.0039044516161084175\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8f7ad99aa14a8c8509d3d76ed87a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/1634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :loss1  loss classifier: 0.0051286290399730206\n",
      "loss contrastive :loss1  loss classifier: 0.003897738875821233\n",
      "Evaluate loss 0.004662337076862775\n",
      "STEP 30000: loss classifier: 0.005140595603734255\n",
      "STEP 30100: loss classifier: 0.0047587184235453606\n",
      "STEP 30200: loss classifier: 0.005003014579415321\n",
      "STEP 30300: loss classifier: 0.003529478795826435\n",
      "STEP 30400: loss classifier: 0.003717755666002631\n",
      "STEP 30500: loss classifier: 0.005274747498333454\n",
      "STEP 30600: loss classifier: 0.0057831620797514915\n",
      "STEP 30700: loss classifier: 0.004950101487338543\n",
      "STEP 30800: loss classifier: 0.0038335053250193596\n",
      "STEP 30900: loss classifier: 0.0038451431319117546\n",
      "STEP 31000: loss classifier: 0.005153605714440346\n",
      "STEP 31100: loss classifier: 0.006822861731052399\n",
      "STEP 31200: loss classifier: 0.004872429650276899\n",
      "STEP 31300: loss classifier: 0.004087185952812433\n",
      "STEP 31400: loss classifier: 0.005991858895868063\n",
      "STEP 31500: loss classifier: 0.004975345451384783\n",
      "STEP 31600: loss classifier: 0.005487479735165834\n",
      "STEP 31700: loss classifier: 0.003201665822416544\n",
      "STEP 31800: loss classifier: 0.0044185081496834755\n",
      "STEP 31900: loss classifier: 0.004878288600593805\n",
      "STEP 32000: loss classifier: 0.005917881149798632\n",
      "STEP 32100: loss classifier: 0.003804215230047703\n",
      "STEP 32200: loss classifier: 0.003976164851337671\n",
      "STEP 32300: loss classifier: 0.0050465865060687065\n",
      "STEP 32400: loss classifier: 0.004576367326080799\n",
      "STEP 32500: loss classifier: 0.0038441787473857403\n",
      "STEP 32600: loss classifier: 0.004302906338125467\n",
      "STEP 32700: loss classifier: 0.006487242877483368\n",
      "STEP 32800: loss classifier: 0.004079633858054876\n",
      "STEP 32900: loss classifier: 0.005126351024955511\n",
      "STEP 33000: loss classifier: 0.006483091972768307\n",
      "STEP 33100: loss classifier: 0.004719126503914595\n",
      "STEP 33200: loss classifier: 0.005190897732973099\n",
      "STEP 33300: loss classifier: 0.003881746670231223\n",
      "STEP 33400: loss classifier: 0.0052955262362957\n",
      "STEP 33500: loss classifier: 0.0033951583318412304\n",
      "STEP 33600: loss classifier: 0.0037379772402346134\n",
      "STEP 33700: loss classifier: 0.005341857671737671\n",
      "STEP 33800: loss classifier: 0.005504686385393143\n",
      "STEP 33900: loss classifier: 0.003546756925061345\n",
      "STEP 34000: loss classifier: 0.005359571892768145\n",
      "STEP 34100: loss classifier: 0.005308416206389666\n",
      "STEP 34200: loss classifier: 0.005642709322273731\n",
      "STEP 34300: loss classifier: 0.004647449590265751\n",
      "STEP 34400: loss classifier: 0.0038230102509260178\n",
      "STEP 34500: loss classifier: 0.003993525635451078\n",
      "STEP 34600: loss classifier: 0.0047010635025799274\n",
      "STEP 34700: loss classifier: 0.0041621592827141285\n",
      "STEP 34800: loss classifier: 0.00332854432053864\n",
      "STEP 34900: loss classifier: 0.004707727115601301\n",
      "STEP 35000: loss classifier: 0.006171169690787792\n",
      "STEP 35100: loss classifier: 0.0036570285446941853\n",
      "STEP 35200: loss classifier: 0.004913306795060635\n",
      "STEP 35300: loss classifier: 0.0038171259220689535\n",
      "STEP 35400: loss classifier: 0.003526105545461178\n",
      "STEP 35500: loss classifier: 0.0058119166642427444\n",
      "STEP 35600: loss classifier: 0.005884627345949411\n",
      "STEP 35700: loss classifier: 0.005058834329247475\n",
      "STEP 35800: loss classifier: 0.00487087108194828\n",
      "STEP 35900: loss classifier: 0.0034024491906166077\n",
      "STEP 36000: loss classifier: 0.004243271891027689\n",
      "STEP 36100: loss classifier: 0.006283887661993504\n",
      "STEP 36200: loss classifier: 0.0066113555803895\n",
      "STEP 36300: loss classifier: 0.004290493670850992\n",
      "STEP 36400: loss classifier: 0.007166322786360979\n",
      "STEP 36500: loss classifier: 0.0032176475506275892\n",
      "STEP 36600: loss classifier: 0.004118143115192652\n",
      "STEP 36700: loss classifier: 0.002966440049931407\n",
      "STEP 36800: loss classifier: 0.00500953895971179\n",
      "STEP 36900: loss classifier: 0.004067226778715849\n",
      "STEP 37000: loss classifier: 0.0031520959455519915\n",
      "STEP 37100: loss classifier: 0.004945349879562855\n",
      "STEP 37200: loss classifier: 0.004485675133764744\n",
      "STEP 37300: loss classifier: 0.005772233474999666\n",
      "STEP 37400: loss classifier: 0.005387640092521906\n",
      "STEP 37500: loss classifier: 0.0048598297871649265\n",
      "STEP 37600: loss classifier: 0.00406272429972887\n",
      "STEP 37700: loss classifier: 0.0037462322507053614\n",
      "STEP 37800: loss classifier: 0.004369313828647137\n",
      "STEP 37900: loss classifier: 0.0039619444869458675\n",
      "STEP 38000: loss classifier: 0.004404722712934017\n",
      "STEP 38100: loss classifier: 0.005146901588886976\n",
      "STEP 38200: loss classifier: 0.0041359793394804\n",
      "STEP 38300: loss classifier: 0.004206749610602856\n",
      "STEP 38400: loss classifier: 0.003490335075184703\n",
      "STEP 38500: loss classifier: 0.005287167150527239\n",
      "STEP 38600: loss classifier: 0.0036068109329789877\n",
      "STEP 38700: loss classifier: 0.0033238844480365515\n",
      "STEP 38800: loss classifier: 0.0030940519645810127\n",
      "STEP 38900: loss classifier: 0.004877639468759298\n",
      "STEP 39000: loss classifier: 0.003833677154034376\n",
      "STEP 39100: loss classifier: 0.005546800326555967\n",
      "STEP 39200: loss classifier: 0.003808568697422743\n",
      "STEP 39300: loss classifier: 0.006247335113584995\n",
      "STEP 39400: loss classifier: 0.003956025466322899\n",
      "STEP 39500: loss classifier: 0.004374841693788767\n",
      "STEP 39600: loss classifier: 0.0053808498196303844\n",
      "STEP 39700: loss classifier: 0.003565568011254072\n",
      "STEP 39800: loss classifier: 0.004572767298668623\n"
     ]
    }
   ],
   "source": [
    "train_loss, _ = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50942b0908f04475849a6ddc8ccc07f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1c7e8d7c91475f80c5e283e267b34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0010610689641907811  loss classifier: 0.6774331331253052\n",
      "STEP 50: loss contrastive :0.0010139003861695528  loss classifier: 0.38425585627555847\n",
      "STEP 100: loss contrastive :0.0009989901445806026  loss classifier: 0.3425288498401642\n",
      "STEP 150: loss contrastive :0.0009738221997395158  loss classifier: 0.312829852104187\n",
      "STEP 200: loss contrastive :0.0009476971463300288  loss classifier: 0.2964755892753601\n",
      "STEP 250: loss contrastive :0.0009283071267418563  loss classifier: 0.29871895909309387\n",
      "STEP 300: loss contrastive :0.0009683565585874021  loss classifier: 0.29144302010536194\n",
      "STEP 350: loss contrastive :0.0009231276926584542  loss classifier: 0.268029123544693\n",
      "STEP 400: loss contrastive :0.0009250261355191469  loss classifier: 0.27633774280548096\n",
      "STEP 450: loss contrastive :0.0009329313761554658  loss classifier: 0.292875736951828\n",
      "STEP 500: loss contrastive :0.0009649590938352048  loss classifier: 0.28385740518569946\n",
      "STEP 550: loss contrastive :0.000941928185056895  loss classifier: 0.2630852460861206\n",
      "STEP 600: loss contrastive :0.0008871428435668349  loss classifier: 0.26286780834198\n",
      "STEP 650: loss contrastive :0.0009311708272434771  loss classifier: 0.24646306037902832\n",
      "STEP 700: loss contrastive :0.0009406199678778648  loss classifier: 0.27898043394088745\n",
      "STEP 750: loss contrastive :0.000936273077968508  loss classifier: 0.2600308954715729\n",
      "STEP 800: loss contrastive :0.0009399568662047386  loss classifier: 0.2894088625907898\n",
      "STEP 850: loss contrastive :0.0009553144918754697  loss classifier: 0.24985231459140778\n",
      "STEP 900: loss contrastive :0.0009723578114062548  loss classifier: 0.2619337737560272\n",
      "STEP 950: loss contrastive :0.0009201912907883525  loss classifier: 0.2603614926338196\n",
      "STEP 1000: loss contrastive :0.0009279796504415572  loss classifier: 0.2986651062965393\n",
      "STEP 1050: loss contrastive :0.0009358207462355494  loss classifier: 0.2539086937904358\n",
      "STEP 1100: loss contrastive :0.0009451773366890848  loss classifier: 0.2548912763595581\n",
      "STEP 1150: loss contrastive :0.0008927385206334293  loss classifier: 0.25713878870010376\n",
      "STEP 1200: loss contrastive :0.0009222639491781592  loss classifier: 0.2555825412273407\n",
      "STEP 1250: loss contrastive :0.0009245303808711469  loss classifier: 0.23886622488498688\n",
      "STEP 1300: loss contrastive :0.0009245075052604079  loss classifier: 0.24136416614055634\n",
      "\n",
      "1334: 0.28489712053693633\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0365cee28b3747578d0294eddf8555b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.0008807076374068856  loss classifier: 0.21857139468193054\n",
      "loss contrastive :0.0009065266931429505  loss classifier: 0.23401078581809998\n",
      "loss contrastive :0.0009554822463542223  loss classifier: 0.24336905777454376\n",
      "loss contrastive :0.0009160935296677053  loss classifier: 0.24640393257141113\n",
      "loss contrastive :0.0009338076924905181  loss classifier: 0.25457191467285156\n",
      "loss contrastive :0.0009518761653453112  loss classifier: 0.24289177358150482\n",
      "loss contrastive :0.0009178954060189426  loss classifier: 0.21809642016887665\n",
      "\n",
      "Evaluate loss 0.239400030033929\n",
      "\n",
      " Epoch 2 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c444f1e5434994a73ab213eca777b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0009442084701731801  loss classifier: 0.2308642864227295\n",
      "STEP 50: loss contrastive :0.0009086896316148341  loss classifier: 0.24797853827476501\n",
      "STEP 100: loss contrastive :0.0008787601254880428  loss classifier: 0.24488425254821777\n",
      "STEP 150: loss contrastive :0.0009530895622447133  loss classifier: 0.2338808923959732\n",
      "STEP 200: loss contrastive :0.000961251906119287  loss classifier: 0.2411338984966278\n",
      "STEP 250: loss contrastive :0.000928273017052561  loss classifier: 0.2574857175350189\n",
      "STEP 300: loss contrastive :0.0008967827307060361  loss classifier: 0.2313375324010849\n",
      "STEP 350: loss contrastive :0.000887261179741472  loss classifier: 0.2302122563123703\n",
      "STEP 400: loss contrastive :0.0009499300504103303  loss classifier: 0.23939299583435059\n",
      "STEP 450: loss contrastive :0.0009526830399408937  loss classifier: 0.24924124777317047\n",
      "STEP 500: loss contrastive :0.0009005170431919396  loss classifier: 0.23633794486522675\n",
      "STEP 550: loss contrastive :0.0008917097002267838  loss classifier: 0.2255229502916336\n",
      "STEP 600: loss contrastive :0.0008761959616094828  loss classifier: 0.22579538822174072\n",
      "STEP 650: loss contrastive :0.0009193914011120796  loss classifier: 0.22465437650680542\n",
      "STEP 700: loss contrastive :0.0009096328285522759  loss classifier: 0.2146378457546234\n",
      "STEP 750: loss contrastive :0.000901732943020761  loss classifier: 0.2276478260755539\n",
      "STEP 800: loss contrastive :0.000923407613299787  loss classifier: 0.22784242033958435\n",
      "STEP 850: loss contrastive :0.0009010914945974946  loss classifier: 0.2391776144504547\n",
      "STEP 900: loss contrastive :0.000927389191929251  loss classifier: 0.21868760883808136\n",
      "STEP 950: loss contrastive :0.0009482597233727574  loss classifier: 0.2268771231174469\n",
      "STEP 1000: loss contrastive :0.0009044851758517325  loss classifier: 0.23813016712665558\n",
      "STEP 1050: loss contrastive :0.0009156233281828463  loss classifier: 0.24004775285720825\n",
      "STEP 1100: loss contrastive :0.0009272374445572495  loss classifier: 0.23553557693958282\n",
      "STEP 1150: loss contrastive :0.0009110203827731311  loss classifier: 0.2299789935350418\n",
      "STEP 1200: loss contrastive :0.0008783993544057012  loss classifier: 0.21437770128250122\n",
      "STEP 1250: loss contrastive :0.000897350546438247  loss classifier: 0.2281295657157898\n",
      "STEP 1300: loss contrastive :0.0009208637056872249  loss classifier: 0.2311241328716278\n",
      "\n",
      "1334: 0.23070132303773688\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0783c7864bc4ce080e785b6c3269491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.0008893839549273252  loss classifier: 0.20146054029464722\n",
      "loss contrastive :0.0008835346670821309  loss classifier: 0.2135966420173645\n",
      "loss contrastive :0.0009539992897771299  loss classifier: 0.22898778319358826\n",
      "loss contrastive :0.0008915557991713285  loss classifier: 0.2200794667005539\n",
      "loss contrastive :0.0009340487304143608  loss classifier: 0.23876436054706573\n",
      "loss contrastive :0.0009146474185399711  loss classifier: 0.22410865128040314\n",
      "loss contrastive :0.0009144307114183903  loss classifier: 0.20079663395881653\n",
      "\n",
      "Evaluate loss 0.224818488742624\n",
      "\n",
      " Epoch 3 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f5cb9bb826451e88a8d01498e0c17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0008946646121330559  loss classifier: 0.215729758143425\n",
      "STEP 50: loss contrastive :0.000933293136768043  loss classifier: 0.21107187867164612\n",
      "STEP 100: loss contrastive :0.0009095509303733706  loss classifier: 0.20472170412540436\n",
      "STEP 150: loss contrastive :0.0008889230666682124  loss classifier: 0.2009972184896469\n",
      "STEP 200: loss contrastive :0.000898425467312336  loss classifier: 0.220720112323761\n",
      "STEP 250: loss contrastive :0.0008432338945567608  loss classifier: 0.19577822089195251\n",
      "STEP 300: loss contrastive :0.0009300133096985519  loss classifier: 0.20456856489181519\n",
      "STEP 350: loss contrastive :0.0009636440081521869  loss classifier: 0.21720096468925476\n",
      "STEP 400: loss contrastive :0.0009122542687691748  loss classifier: 0.21185627579689026\n",
      "STEP 450: loss contrastive :0.0009121580515056849  loss classifier: 0.2047554850578308\n",
      "STEP 500: loss contrastive :0.0009195298771373928  loss classifier: 0.19961179792881012\n",
      "STEP 550: loss contrastive :0.0009489330695942044  loss classifier: 0.22855181992053986\n",
      "STEP 600: loss contrastive :0.0009001567377708852  loss classifier: 0.2121484875679016\n",
      "STEP 650: loss contrastive :0.0009109622915275395  loss classifier: 0.23586995899677277\n",
      "STEP 700: loss contrastive :0.0008985733147710562  loss classifier: 0.20237237215042114\n",
      "STEP 750: loss contrastive :0.0008942872518673539  loss classifier: 0.19298163056373596\n",
      "STEP 800: loss contrastive :0.0008995592361316085  loss classifier: 0.2190903127193451\n",
      "STEP 850: loss contrastive :0.000899295904673636  loss classifier: 0.19127169251441956\n",
      "STEP 900: loss contrastive :0.0008805564721114933  loss classifier: 0.19340704381465912\n",
      "STEP 950: loss contrastive :0.0009056915296241641  loss classifier: 0.21813267469406128\n",
      "STEP 1000: loss contrastive :0.0008682067273184657  loss classifier: 0.2132941633462906\n",
      "STEP 1050: loss contrastive :0.0008939316030591726  loss classifier: 0.17876087129116058\n",
      "STEP 1100: loss contrastive :0.0009004317107610404  loss classifier: 0.2173953354358673\n",
      "STEP 1150: loss contrastive :0.0009163804352283478  loss classifier: 0.20730827748775482\n",
      "STEP 1200: loss contrastive :0.0009063553297892213  loss classifier: 0.187158465385437\n",
      "STEP 1250: loss contrastive :0.0009123301133513451  loss classifier: 0.1970745474100113\n",
      "STEP 1300: loss contrastive :0.0008618412539362907  loss classifier: 0.19862429797649384\n",
      "\n",
      "1334: 0.2060488157727745\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7792f004dd134b61a5f50a0064e33fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.0008923079585656524  loss classifier: 0.2057870328426361\n",
      "loss contrastive :0.0008866586722433567  loss classifier: 0.21777421236038208\n",
      "loss contrastive :0.0009292339673265815  loss classifier: 0.218560591340065\n",
      "loss contrastive :0.0008858245564624667  loss classifier: 0.2149192988872528\n",
      "loss contrastive :0.0009333193302154541  loss classifier: 0.24170167744159698\n",
      "loss contrastive :0.0009262507664971054  loss classifier: 0.21084506809711456\n",
      "loss contrastive :0.0009197652107104659  loss classifier: 0.18989942967891693\n",
      "\n",
      "Evaluate loss 0.22155216634273528\n",
      "\n",
      " Epoch 4 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d11340fdfcd4fe0aa969caa59b20ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0009148622630164027  loss classifier: 0.18414299190044403\n",
      "STEP 50: loss contrastive :0.000876838224940002  loss classifier: 0.16890180110931396\n",
      "STEP 100: loss contrastive :0.0008993896190077066  loss classifier: 0.16637739539146423\n",
      "STEP 150: loss contrastive :0.0009161243215203285  loss classifier: 0.1544165462255478\n",
      "STEP 200: loss contrastive :0.0008454859489575028  loss classifier: 0.19036753475666046\n",
      "STEP 250: loss contrastive :0.0008776029571890831  loss classifier: 0.17521975934505463\n",
      "STEP 300: loss contrastive :0.000863123219460249  loss classifier: 0.15643495321273804\n",
      "STEP 350: loss contrastive :0.0008465180872008204  loss classifier: 0.17740429937839508\n",
      "STEP 400: loss contrastive :0.0008260203758254647  loss classifier: 0.17282404005527496\n",
      "STEP 450: loss contrastive :0.0008817882044240832  loss classifier: 0.18425744771957397\n",
      "STEP 500: loss contrastive :0.0008810520521365106  loss classifier: 0.17513351142406464\n",
      "STEP 550: loss contrastive :0.0008323629735969007  loss classifier: 0.17659255862236023\n",
      "STEP 600: loss contrastive :0.0008939534891396761  loss classifier: 0.1733613759279251\n",
      "STEP 650: loss contrastive :0.0009149214019998908  loss classifier: 0.18991927802562714\n",
      "STEP 700: loss contrastive :0.0009140722686424851  loss classifier: 0.17144525051116943\n",
      "STEP 750: loss contrastive :0.0008822049712762237  loss classifier: 0.1800699084997177\n",
      "STEP 800: loss contrastive :0.0008755988092161715  loss classifier: 0.18652907013893127\n",
      "STEP 850: loss contrastive :0.0008660417515784502  loss classifier: 0.16443268954753876\n",
      "STEP 1150: loss contrastive :0.0008580789435654879  loss classifier: 0.18766213953495026\n",
      "STEP 1200: loss contrastive :0.0008628497598692775  loss classifier: 0.1909981667995453\n",
      "STEP 1250: loss contrastive :0.0008643883047625422  loss classifier: 0.1847379058599472\n",
      "STEP 1300: loss contrastive :0.0009039732394739985  loss classifier: 0.17725397646427155\n",
      "\n",
      "1334: 0.17716465605778642\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b85f27d5bb84cfb92817115be50fbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.000891766743734479  loss classifier: 0.20928114652633667\n",
      "loss contrastive :0.0008764481754042208  loss classifier: 0.2221929430961609\n",
      "loss contrastive :0.0009337436640635133  loss classifier: 0.22371983528137207\n",
      "loss contrastive :0.0008815022883936763  loss classifier: 0.21296896040439606\n",
      "loss contrastive :0.0009235467296093702  loss classifier: 0.2502913475036621\n",
      "loss contrastive :0.000922387174796313  loss classifier: 0.22078891098499298\n",
      "loss contrastive :0.000905571854673326  loss classifier: 0.1916544884443283\n",
      "\n",
      "Evaluate loss 0.2295128060238702\n",
      "\n",
      " Epoch 5 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e8451c1ed44481a879241e817b41d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0008306929375976324  loss classifier: 0.15690772235393524\n",
      "STEP 50: loss contrastive :0.0008291496778838336  loss classifier: 0.15071210265159607\n",
      "STEP 100: loss contrastive :0.0008183603640645742  loss classifier: 0.1562301069498062\n",
      "STEP 150: loss contrastive :0.0008440852398052812  loss classifier: 0.14083687961101532\n",
      "STEP 200: loss contrastive :0.000838672393001616  loss classifier: 0.1721179187297821\n",
      "STEP 250: loss contrastive :0.0008693318231962621  loss classifier: 0.15354199707508087\n",
      "STEP 300: loss contrastive :0.0008761153439991176  loss classifier: 0.1530335247516632\n",
      "STEP 350: loss contrastive :0.0007590079912915826  loss classifier: 0.12554311752319336\n",
      "STEP 400: loss contrastive :0.0008974550291895866  loss classifier: 0.15950292348861694\n",
      "STEP 450: loss contrastive :0.0008219852461479604  loss classifier: 0.17826594412326813\n",
      "STEP 500: loss contrastive :0.0008417931385338306  loss classifier: 0.14752903580665588\n",
      "STEP 550: loss contrastive :0.0008218636503443122  loss classifier: 0.16200672090053558\n",
      "STEP 600: loss contrastive :0.0008211528765968978  loss classifier: 0.13816477358341217\n",
      "STEP 650: loss contrastive :0.0008447055006399751  loss classifier: 0.16482867300510406\n",
      "STEP 700: loss contrastive :0.0008477813680656254  loss classifier: 0.1468813568353653\n",
      "STEP 750: loss contrastive :0.0008151968941092491  loss classifier: 0.12947943806648254\n",
      "STEP 800: loss contrastive :0.0008598427521064878  loss classifier: 0.13776153326034546\n",
      "STEP 850: loss contrastive :0.0008039619424380362  loss classifier: 0.14150801301002502\n",
      "STEP 900: loss contrastive :0.0007994532352313399  loss classifier: 0.14632833003997803\n",
      "STEP 950: loss contrastive :0.0007750615477561951  loss classifier: 0.1424008458852768\n",
      "STEP 1000: loss contrastive :0.0008294120198115706  loss classifier: 0.14283975958824158\n",
      "STEP 1050: loss contrastive :0.0008441192330792546  loss classifier: 0.12468162178993225\n",
      "STEP 1100: loss contrastive :0.0008190621738322079  loss classifier: 0.1525908261537552\n",
      "STEP 1150: loss contrastive :0.0008136738324537873  loss classifier: 0.15397946536540985\n",
      "STEP 1200: loss contrastive :0.0008062416454777122  loss classifier: 0.15165288746356964\n",
      "STEP 1250: loss contrastive :0.0007563012186437845  loss classifier: 0.11864683032035828\n",
      "STEP 1300: loss contrastive :0.0008251076797023416  loss classifier: 0.1428585648536682\n",
      "\n",
      "1334: 0.14450837763842572\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edfd2cfb50346ed9b9b205f9f85e50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.0008922904962673783  loss classifier: 0.22516782581806183\n",
      "loss contrastive :0.0008801620570011437  loss classifier: 0.2342340350151062\n",
      "loss contrastive :0.0009436492691747844  loss classifier: 0.2321746051311493\n",
      "loss contrastive :0.0008874292252585292  loss classifier: 0.2229316383600235\n",
      "loss contrastive :0.0009319459786638618  loss classifier: 0.2601303458213806\n",
      "loss contrastive :0.0009146207594312727  loss classifier: 0.2369941771030426\n",
      "loss contrastive :0.0009042565943673253  loss classifier: 0.2024374008178711\n",
      "\n",
      "Evaluate loss 0.24233244167906898\n",
      "\n",
      "CPU times: user 55min 33s, sys: 6min 10s, total: 1h 1min 44s\n",
      "Wall time: 56min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set initial loss to infinite\n",
    "import time\n",
    "best_valid_loss = float('inf')\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "now=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "#for each epoch\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, EPOCHS))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _, _ = evaluate(val_dataloader)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f\"./Baseline_abstract_model{now}.pt\")\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081db7d577cc4a6d8095499e331cbe81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65c962f73d848efbaec53b05a597576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0011736995074898005  loss classifier: 0.6784937977790833\n",
      "STEP 50: loss contrastive :0.0010244036093354225  loss classifier: 0.38525381684303284\n",
      "STEP 100: loss contrastive :0.0010459651239216328  loss classifier: 0.34234097599983215\n",
      "STEP 150: loss contrastive :0.0010059644700959325  loss classifier: 0.31425362825393677\n",
      "STEP 200: loss contrastive :0.0009743745322339237  loss classifier: 0.29644834995269775\n",
      "STEP 250: loss contrastive :0.000911412644200027  loss classifier: 0.3009869456291199\n",
      "STEP 300: loss contrastive :0.000994911533780396  loss classifier: 0.2920014560222626\n",
      "STEP 350: loss contrastive :0.0009435606189072132  loss classifier: 0.27130696177482605\n",
      "STEP 550: loss contrastive :0.0009513312834315002  loss classifier: 0.2624566853046417\n",
      "STEP 600: loss contrastive :0.0008490897598676383  loss classifier: 0.2606485188007355\n",
      "STEP 650: loss contrastive :0.0009443267481401563  loss classifier: 0.2466069906949997\n",
      "STEP 700: loss contrastive :0.0009472237434238195  loss classifier: 0.2801697552204132\n",
      "STEP 750: loss contrastive :0.0009368840837851167  loss classifier: 0.2562132775783539\n",
      "STEP 800: loss contrastive :0.0009517501457594335  loss classifier: 0.2913571000099182\n",
      "STEP 850: loss contrastive :0.0010120428632944822  loss classifier: 0.2502491772174835\n",
      "STEP 900: loss contrastive :0.001008233637548983  loss classifier: 0.26180511713027954\n",
      "STEP 950: loss contrastive :0.0008975267992354929  loss classifier: 0.2535405158996582\n",
      "STEP 1000: loss contrastive :0.0009393665241077542  loss classifier: 0.29835718870162964\n",
      "STEP 1050: loss contrastive :0.0009370642947033048  loss classifier: 0.25545239448547363\n",
      "STEP 1100: loss contrastive :0.000973925634752959  loss classifier: 0.250112920999527\n",
      "STEP 1150: loss contrastive :0.0008612530073150992  loss classifier: 0.26104840636253357\n",
      "STEP 1200: loss contrastive :0.0008924937574192882  loss classifier: 0.2545377314090729\n",
      "STEP 1250: loss contrastive :0.0009421527502126992  loss classifier: 0.24482595920562744\n",
      "STEP 1300: loss contrastive :0.0009768663439899683  loss classifier: 0.24315299093723297\n",
      "\n",
      "1334: 0.2857341812456145\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e05d66746d463dac4356dd2fc3b1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.00086324627045542  loss classifier: 0.22190608084201813\n",
      "loss contrastive :0.0008816148620098829  loss classifier: 0.23421892523765564\n",
      "loss contrastive :0.0010400454048067331  loss classifier: 0.2457604855298996\n",
      "loss contrastive :0.0009191778372041881  loss classifier: 0.2457519769668579\n",
      "loss contrastive :0.0009707127464935184  loss classifier: 0.25364264845848083\n",
      "loss contrastive :0.0009471081430092454  loss classifier: 0.23740224540233612\n",
      "loss contrastive :0.000926076085306704  loss classifier: 0.22345615923404694\n",
      "\n",
      "Evaluate loss 0.23963707110711507\n",
      "\n",
      " Epoch 2 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb08dd1e706547359f9591ee96b74750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0009488399373367429  loss classifier: 0.23232369124889374\n",
      "STEP 50: loss contrastive :0.0008958855760283768  loss classifier: 0.253220796585083\n",
      "STEP 100: loss contrastive :0.0008832403691485524  loss classifier: 0.2489113211631775\n",
      "STEP 150: loss contrastive :0.0009393055224791169  loss classifier: 0.23295415937900543\n",
      "STEP 200: loss contrastive :0.0009999671019613743  loss classifier: 0.2376948744058609\n",
      "STEP 250: loss contrastive :0.0009627904510125518  loss classifier: 0.25691667199134827\n",
      "STEP 300: loss contrastive :0.0008979059057310224  loss classifier: 0.2328859567642212\n",
      "STEP 350: loss contrastive :0.0008929578470997512  loss classifier: 0.22803781926631927\n",
      "STEP 400: loss contrastive :0.0010320055298507214  loss classifier: 0.24067842960357666\n",
      "STEP 450: loss contrastive :0.0009719012305140495  loss classifier: 0.2511763274669647\n",
      "STEP 500: loss contrastive :0.0009282968239858747  loss classifier: 0.24083101749420166\n",
      "STEP 550: loss contrastive :0.0008754656882956624  loss classifier: 0.2232983559370041\n",
      "STEP 600: loss contrastive :0.0008369518909603357  loss classifier: 0.227905735373497\n",
      "STEP 650: loss contrastive :0.0009327534353360534  loss classifier: 0.22973594069480896\n",
      "STEP 700: loss contrastive :0.000927383778616786  loss classifier: 0.21701186895370483\n",
      "STEP 750: loss contrastive :0.0008915961370803416  loss classifier: 0.23167774081230164\n",
      "STEP 800: loss contrastive :0.0009137419983744621  loss classifier: 0.23212331533432007\n",
      "STEP 850: loss contrastive :0.0008943690918385983  loss classifier: 0.23854492604732513\n",
      "STEP 900: loss contrastive :0.0009501170716248453  loss classifier: 0.21960677206516266\n",
      "STEP 950: loss contrastive :0.0009552448755130172  loss classifier: 0.2359723597764969\n",
      "STEP 1000: loss contrastive :0.000886697496753186  loss classifier: 0.23530149459838867\n",
      "STEP 1050: loss contrastive :0.0009854263626039028  loss classifier: 0.24442051351070404\n",
      "STEP 1100: loss contrastive :0.0009401102433912456  loss classifier: 0.23766712844371796\n",
      "STEP 1150: loss contrastive :0.000903163687326014  loss classifier: 0.23047256469726562\n",
      "STEP 1200: loss contrastive :0.0008468367159366608  loss classifier: 0.21468748152256012\n",
      "STEP 1250: loss contrastive :0.0008843366522341967  loss classifier: 0.2243933528661728\n",
      "STEP 1300: loss contrastive :0.0009589018300175667  loss classifier: 0.23291292786598206\n",
      "\n",
      "1334: 0.23178507207931204\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c614c6adb7c41878b21a5bf808e2807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.0008599188877269626  loss classifier: 0.2006521224975586\n",
      "loss contrastive :0.0008460755925625563  loss classifier: 0.2135000228881836\n",
      "loss contrastive :0.000979618402197957  loss classifier: 0.22505564987659454\n",
      "loss contrastive :0.0008922427077777684  loss classifier: 0.22128352522850037\n",
      "loss contrastive :0.0010183851700276136  loss classifier: 0.24932970106601715\n",
      "loss contrastive :0.000922065693885088  loss classifier: 0.22875399887561798\n",
      "loss contrastive :0.0009104629280045629  loss classifier: 0.19750604033470154\n",
      "\n",
      "Evaluate loss 0.22413069563252586\n",
      "\n",
      " Epoch 3 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b3a84c364d467e84459a7181919547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0008980465936474502  loss classifier: 0.22032979130744934\n",
      "STEP 50: loss contrastive :0.0009284426923841238  loss classifier: 0.20918573439121246\n",
      "STEP 100: loss contrastive :0.000885294983163476  loss classifier: 0.2093435525894165\n",
      "STEP 150: loss contrastive :0.0008536387467756867  loss classifier: 0.2002076506614685\n",
      "STEP 200: loss contrastive :0.0008916630176827312  loss classifier: 0.22726105153560638\n",
      "STEP 250: loss contrastive :0.0008259156020358205  loss classifier: 0.1979772448539734\n",
      "STEP 300: loss contrastive :0.0009368800092488527  loss classifier: 0.2023366093635559\n",
      "STEP 350: loss contrastive :0.0009978469461202621  loss classifier: 0.22486214339733124\n",
      "STEP 400: loss contrastive :0.0009512822143733501  loss classifier: 0.21411678194999695\n",
      "STEP 450: loss contrastive :0.0009087007492780685  loss classifier: 0.20623838901519775\n",
      "STEP 500: loss contrastive :0.0009110539685934782  loss classifier: 0.19071035087108612\n",
      "STEP 550: loss contrastive :0.001022130949422717  loss classifier: 0.22776824235916138\n",
      "STEP 600: loss contrastive :0.000917917350307107  loss classifier: 0.222357839345932\n",
      "STEP 650: loss contrastive :0.0009150479454547167  loss classifier: 0.22849297523498535\n",
      "STEP 700: loss contrastive :0.0008697551675140858  loss classifier: 0.2075357884168625\n",
      "STEP 750: loss contrastive :0.0008842159295454621  loss classifier: 0.2006600797176361\n",
      "STEP 800: loss contrastive :0.000907926878426224  loss classifier: 0.2176223248243332\n",
      "STEP 850: loss contrastive :0.0008957922109402716  loss classifier: 0.19320987164974213\n",
      "STEP 900: loss contrastive :0.0008517869282513857  loss classifier: 0.19495932757854462\n",
      "STEP 950: loss contrastive :0.0009135412983596325  loss classifier: 0.21300384402275085\n",
      "STEP 1000: loss contrastive :0.0008291336707770824  loss classifier: 0.22137853503227234\n",
      "STEP 1050: loss contrastive :0.0008741163183003664  loss classifier: 0.1819954812526703\n",
      "STEP 1100: loss contrastive :0.0009068308863788843  loss classifier: 0.21869130432605743\n",
      "STEP 1150: loss contrastive :0.0009539425373077393  loss classifier: 0.20992732048034668\n",
      "STEP 1200: loss contrastive :0.0009187277755700052  loss classifier: 0.1884355992078781\n",
      "STEP 1250: loss contrastive :0.0009197737090289593  loss classifier: 0.19881734251976013\n",
      "STEP 1300: loss contrastive :0.0008159006247296929  loss classifier: 0.20219449698925018\n",
      "\n",
      "1334: 0.20851367177364977\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fbe53b7b54489896e3af142acdcb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.0008725500083528459  loss classifier: 0.20578263700008392\n",
      "loss contrastive :0.000910047092474997  loss classifier: 0.21890391409397125\n",
      "loss contrastive :0.0009645494865253568  loss classifier: 0.2212047576904297\n",
      "loss contrastive :0.0009276264463551342  loss classifier: 0.2225142866373062\n",
      "loss contrastive :0.0010248979087918997  loss classifier: 0.24798201024532318\n",
      "loss contrastive :0.0009418590925633907  loss classifier: 0.21279238164424896\n",
      "loss contrastive :0.0009347681188955903  loss classifier: 0.18822413682937622\n",
      "\n",
      "Evaluate loss 0.22210357636213302\n",
      "\n",
      " Epoch 4 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef069f9af29469588ebbd7c6c36c103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.000942211365327239  loss classifier: 0.18686386942863464\n",
      "STEP 50: loss contrastive :0.0008771411376073956  loss classifier: 0.168535053730011\n",
      "STEP 100: loss contrastive :0.0009232746670022607  loss classifier: 0.17536720633506775\n",
      "STEP 150: loss contrastive :0.0009367841412313282  loss classifier: 0.15939436852931976\n",
      "STEP 200: loss contrastive :0.0008358241757377982  loss classifier: 0.20461532473564148\n",
      "STEP 250: loss contrastive :0.0008678673766553402  loss classifier: 0.176971897482872\n",
      "STEP 300: loss contrastive :0.000818393484223634  loss classifier: 0.15583422780036926\n",
      "STEP 350: loss contrastive :0.0008111900533549488  loss classifier: 0.176048144698143\n",
      "STEP 400: loss contrastive :0.0007947029080241919  loss classifier: 0.17967219650745392\n",
      "STEP 450: loss contrastive :0.0008485829457640648  loss classifier: 0.18107275664806366\n",
      "STEP 500: loss contrastive :0.000850270502269268  loss classifier: 0.17511844635009766\n",
      "STEP 550: loss contrastive :0.0008164546452462673  loss classifier: 0.18557403981685638\n",
      "STEP 600: loss contrastive :0.0009128558449447155  loss classifier: 0.18678396940231323\n",
      "STEP 650: loss contrastive :0.0009649867424741387  loss classifier: 0.19543366134166718\n",
      "STEP 700: loss contrastive :0.0009263024548999965  loss classifier: 0.18813340365886688\n",
      "STEP 750: loss contrastive :0.0008957870886661112  loss classifier: 0.18359319865703583\n",
      "STEP 800: loss contrastive :0.0008358509512618184  loss classifier: 0.18222005665302277\n",
      "STEP 850: loss contrastive :0.0008834587060846388  loss classifier: 0.17140616476535797\n",
      "STEP 900: loss contrastive :0.0008425001287832856  loss classifier: 0.19489166140556335\n",
      "STEP 950: loss contrastive :0.0009731538593769073  loss classifier: 0.18568919599056244\n",
      "STEP 1000: loss contrastive :0.0008747252286411822  loss classifier: 0.1568465679883957\n",
      "STEP 1050: loss contrastive :0.0008505711448378861  loss classifier: 0.19135193526744843\n",
      "STEP 1100: loss contrastive :0.0009187135146930814  loss classifier: 0.1793641597032547\n",
      "STEP 1150: loss contrastive :0.0008155541145242751  loss classifier: 0.18359552323818207\n",
      "STEP 1200: loss contrastive :0.0008373362361453474  loss classifier: 0.18632934987545013\n",
      "STEP 1250: loss contrastive :0.0008826848934404552  loss classifier: 0.18996219336986542\n",
      "STEP 1300: loss contrastive :0.0009075968409888446  loss classifier: 0.17624488472938538\n",
      "\n",
      "1334: 0.18130031145467293\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddb8ba18f1847f8a572667163d27b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.0008937601232901216  loss classifier: 0.2024717777967453\n",
      "loss contrastive :0.0009234985336661339  loss classifier: 0.22096052765846252\n",
      "loss contrastive :0.0009677024208940566  loss classifier: 0.22529539465904236\n",
      "loss contrastive :0.0009381117415614426  loss classifier: 0.21840733289718628\n",
      "loss contrastive :0.001024402561597526  loss classifier: 0.25764214992523193\n",
      "loss contrastive :0.000994328293018043  loss classifier: 0.2228819578886032\n",
      "loss contrastive :0.000939505931455642  loss classifier: 0.1843181699514389\n",
      "\n",
      "Evaluate loss 0.22800866003547396\n",
      "\n",
      " Epoch 5 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975dab97c06646138119634ba1742b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1335.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: loss contrastive :0.0007900340715423226  loss classifier: 0.15494336187839508\n",
      "STEP 50: loss contrastive :0.0008160884608514607  loss classifier: 0.15528662502765656\n",
      "STEP 100: loss contrastive :0.0008477926021441817  loss classifier: 0.16335934400558472\n",
      "STEP 150: loss contrastive :0.0008415866177529097  loss classifier: 0.14178107678890228\n",
      "STEP 200: loss contrastive :0.0008564976742491126  loss classifier: 0.18279756605625153\n",
      "STEP 250: loss contrastive :0.0008687674999237061  loss classifier: 0.1595715433359146\n",
      "STEP 300: loss contrastive :0.0009081882890313864  loss classifier: 0.16121934354305267\n",
      "STEP 350: loss contrastive :0.0006919500301592052  loss classifier: 0.13182401657104492\n",
      "STEP 400: loss contrastive :0.0008912598132155836  loss classifier: 0.15460121631622314\n",
      "STEP 450: loss contrastive :0.0007779154693707824  loss classifier: 0.1831590086221695\n",
      "STEP 500: loss contrastive :0.0008356108446605504  loss classifier: 0.16439509391784668\n",
      "STEP 550: loss contrastive :0.0008153213420882821  loss classifier: 0.15982994437217712\n",
      "STEP 600: loss contrastive :0.0007930166320875287  loss classifier: 0.1499931961297989\n",
      "STEP 650: loss contrastive :0.0008776435279287398  loss classifier: 0.17778578400611877\n",
      "STEP 700: loss contrastive :0.000851315853651613  loss classifier: 0.15625931322574615\n",
      "STEP 750: loss contrastive :0.0007686999160796404  loss classifier: 0.1308041363954544\n",
      "STEP 800: loss contrastive :0.0008525163866579533  loss classifier: 0.1466894894838333\n",
      "STEP 850: loss contrastive :0.0007661130512133241  loss classifier: 0.14178267121315002\n",
      "STEP 900: loss contrastive :0.0007489171694032848  loss classifier: 0.15259340405464172\n",
      "STEP 950: loss contrastive :0.0007311141234822571  loss classifier: 0.14152930676937103\n",
      "STEP 1000: loss contrastive :0.0008320212946273386  loss classifier: 0.15096232295036316\n",
      "STEP 1050: loss contrastive :0.0008164399187080562  loss classifier: 0.13335980474948883\n",
      "STEP 1100: loss contrastive :0.0007713154191151261  loss classifier: 0.14753103256225586\n",
      "STEP 1150: loss contrastive :0.0008120735874399543  loss classifier: 0.16490334272384644\n",
      "STEP 1200: loss contrastive :0.0007694274536333978  loss classifier: 0.14890806376934052\n",
      "STEP 1250: loss contrastive :0.0006670518778264523  loss classifier: 0.1237284392118454\n",
      "STEP 1300: loss contrastive :0.0008364875102415681  loss classifier: 0.15063519775867462\n",
      "\n",
      "1334: 0.1502600369381994\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e893d67efe0436db9a49450f4c988df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval', max=70.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :0.0009505008347332478  loss classifier: 0.21422308683395386\n",
      "loss contrastive :0.000962299294769764  loss classifier: 0.22596541047096252\n",
      "loss contrastive :0.0009981528855860233  loss classifier: 0.2330978512763977\n",
      "loss contrastive :0.000995664857327938  loss classifier: 0.22896067798137665\n",
      "loss contrastive :0.00106520252302289  loss classifier: 0.26746976375579834\n",
      "loss contrastive :0.0010346363997086883  loss classifier: 0.23680955171585083\n",
      "loss contrastive :0.0009839585982263088  loss classifier: 0.19763624668121338\n",
      "\n",
      "Evaluate loss 0.24067499275718415\n",
      "\n",
      "CPU times: user 1h 10min 51s, sys: 17min 12s, total: 1h 28min 4s\n",
      "Wall time: 1h 22min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set initial loss to infinite\n",
    "import time\n",
    "best_valid_loss = float('inf')\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "now=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "#for each epoch\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, EPOCHS))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _, _ = evaluate(val_dataloader)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f\"./Baseline_abstract_model{now}.pt\")\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score)\n",
    "\n",
    "\n",
    "true = np.array(total_labels)\n",
    "pred = np.array(total_preds>0.5)\n",
    "\n",
    "dic = {\n",
    "    \"Accuracy\" : accuracy_score(true,pred),\n",
    "    \"Precision-micro\" : precision_score(true,pred,average='micro'),\n",
    "    \"Precision-macro\" : precision_score(true,pred,average='macro'),\n",
    "    \"recall-micro\" : recall_score(true,pred,average='micro'),\n",
    "    \"recall-macro\" : recall_score(true,pred,average='macro'),\n",
    "    \"f1_micro\" : f1_score(true,pred,average='micro'),\n",
    "    \"f1-macro\" : f1_score(true,pred,average='macro')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.40685096153846156,\n",
       " 'Precision-micro': 0.8202037351443124,\n",
       " 'Precision-macro': 0.43446532160346085,\n",
       " 'recall-micro': 0.5257945145842403,\n",
       " 'recall-macro': 0.26941306850665814,\n",
       " 'f1_micro': 0.6408011672635628,\n",
       " 'f1-macro': 0.3154646223318294}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"./model/CoPatE.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_size = 8时的模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model/Classfication_Baseline_claims_model2022-02-22 01:57:06.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_feather(\"2021-sample-5000_retrieval_two.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [A hanging pocket for point-of-sale display, t...\n",
       "1       [A method for growing a crystal, comprising:we...\n",
       "2       [A device for monitoring usage of a toothbrush...\n",
       "3       [A steerable catheter robotic system, comprisi...\n",
       "4       [A battery-powered device comprising:a power i...\n",
       "                              ...                        \n",
       "4995    [A method of making a carbon nanotube composit...\n",
       "4996    [A storage system comprising a first storage b...\n",
       "4997    [A shoulder implant system comprising:a glenoi...\n",
       "4998    [A noise cancelling soundbar, comprising:one o...\n",
       "4999    [A method for transforming an algal chloroplas...\n",
       "Name: claims, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f32e79a97a34248a4ac07b49fae9fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss contrastive :loss1  loss classifier: 0.0064818705432116985\n",
      "Evaluate loss 0.0064623038514326206\n"
     ]
    }
   ],
   "source": [
    "test_dataset = PatentDataset(\n",
    "  test_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT,\n",
    "  test = True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,drop_last=True)\n",
    "\n",
    "avg_loss, total_preds, total_labels = evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "164dc99c25814f6ab80b1620d5bbdccb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_5445e44543df4f0593767b43681f48f0",
       "max": 86055,
       "style": "IPY_MODEL_ccb7249e92c14924821f89b126e0cacd",
       "value": 85982
      }
     },
     "1d50adeb23e542a5aa98f7de96d6a9d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1e895d0af15b4ff2a6c2ad14a91f5748": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_c903ce8ed8c54437b67158c54da3a22a",
       "max": 10757,
       "style": "IPY_MODEL_f8a513de577440318b252bef4f2da141",
       "value": 9544
      }
     },
     "20a637e61a934dc3a1c79ed24c7cecd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6ec90b1f56454be6b23a9bbfc75ffcf1",
       "style": "IPY_MODEL_6d5cbecbab2a45b28710fe24e4ac6de6",
       "value": " 85982/86055 [11:24:22&lt;00:35,  2.06it/s]"
      }
     },
     "2748d0831de24a3288e63ff82496a7d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_90cfa066ef354ea2b1ee38ef9a5f8099",
       "max": 5,
       "style": "IPY_MODEL_593f5fe9c83d4b85a4fbd9fd447b8e9a"
      }
     },
     "2b90dffa457c429f99c92d510b048761": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "309c7152a11d4c1381a88e9c451751c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "37e133be36cf4b31a6e83448ade7534b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_728b2a6ea0f0465282eb7464e28b8f85",
       "max": 86055,
       "style": "IPY_MODEL_1d50adeb23e542a5aa98f7de96d6a9d7",
       "value": 44175
      }
     },
     "3896630326114e08b072c857b038bc1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3a7d8d6da3f54f238b0afea6e7ee2b85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a467379af6fb4610af04419257bcd97e",
       "style": "IPY_MODEL_2b90dffa457c429f99c92d510b048761",
       "value": "Eval:  89%"
      }
     },
     "3ae8ae0bda93481196c4f129a01bf000": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f50ae3f29d31451f941bca0d4a03a215",
       "style": "IPY_MODEL_8ee2175173904a2eb2d9aba4753e15d6",
       "value": " 44175/86055 [5:52:16&lt;4:55:35,  2.36it/s]"
      }
     },
     "4314a58991154b8e862b9b2e0aebef2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f12a82db3205424182376a8b167778d1",
        "IPY_MODEL_37e133be36cf4b31a6e83448ade7534b",
        "IPY_MODEL_3ae8ae0bda93481196c4f129a01bf000"
       ],
       "layout": "IPY_MODEL_3896630326114e08b072c857b038bc1a"
      }
     },
     "434444785a7e447e87e0cad98313ad1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5445e44543df4f0593767b43681f48f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "556f902dd03b49fbb6bf7120e444631e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "573e3df279c94dcdbcca6d4fbc2baba5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "593f5fe9c83d4b85a4fbd9fd447b8e9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6c8675f1019348b090dcfd9a695e28ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6d5cbecbab2a45b28710fe24e4ac6de6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6ec90b1f56454be6b23a9bbfc75ffcf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ef42ab2cf214b2a9c161320cab5eaf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e5da622f865b4be589d33d9bf11bff95",
       "style": "IPY_MODEL_309c7152a11d4c1381a88e9c451751c9",
       "value": "  0%"
      }
     },
     "728b2a6ea0f0465282eb7464e28b8f85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8ee2175173904a2eb2d9aba4753e15d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "90cfa066ef354ea2b1ee38ef9a5f8099": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9820aa3240884437b73ec6ac97f9bfeb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a7a893bdc5b4996bb0af662499a1031": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3a7d8d6da3f54f238b0afea6e7ee2b85",
        "IPY_MODEL_1e895d0af15b4ff2a6c2ad14a91f5748",
        "IPY_MODEL_d8a58dc7a14847c380437b483941f7b8"
       ],
       "layout": "IPY_MODEL_bb728dc1babb44c59b277d6c0fae77d1"
      }
     },
     "9acb681f98d74355b0d50fd3392c7ec2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a3685fa7e19d4cc78a84fe0f9a47d392": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a467379af6fb4610af04419257bcd97e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ad8d9ad1431f4ab2961b70feb3bffef4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bb728dc1babb44c59b277d6c0fae77d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c0bbf35d93224082a7f0abddb98cecf8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c15c1985d6914f08b96560a7231ee0ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a3685fa7e19d4cc78a84fe0f9a47d392",
       "style": "IPY_MODEL_434444785a7e447e87e0cad98313ad1f",
       "value": " 0/5 [00:00&lt;?, ?it/s]"
      }
     },
     "c903ce8ed8c54437b67158c54da3a22a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ccb7249e92c14924821f89b126e0cacd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d5e99427fc1a466589e96bf94e2e0b89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6ef42ab2cf214b2a9c161320cab5eaf4",
        "IPY_MODEL_2748d0831de24a3288e63ff82496a7d9",
        "IPY_MODEL_c15c1985d6914f08b96560a7231ee0ae"
       ],
       "layout": "IPY_MODEL_c0bbf35d93224082a7f0abddb98cecf8"
      }
     },
     "d8a58dc7a14847c380437b483941f7b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ad8d9ad1431f4ab2961b70feb3bffef4",
       "style": "IPY_MODEL_f1587b8254aa4fe995d0262ff5c0ff0d",
       "value": " 9544/10757 [25:20&lt;03:20,  6.06it/s]"
      }
     },
     "e5da622f865b4be589d33d9bf11bff95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6911c72c9fb4701bf09e05c358cc971": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f16d3926bf7d42f3a864c8aaf75d8fa2",
        "IPY_MODEL_164dc99c25814f6ab80b1620d5bbdccb",
        "IPY_MODEL_20a637e61a934dc3a1c79ed24c7cecd1"
       ],
       "layout": "IPY_MODEL_573e3df279c94dcdbcca6d4fbc2baba5"
      }
     },
     "f12a82db3205424182376a8b167778d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9820aa3240884437b73ec6ac97f9bfeb",
       "style": "IPY_MODEL_6c8675f1019348b090dcfd9a695e28ca",
       "value": "Train:  49%"
      }
     },
     "f1587b8254aa4fe995d0262ff5c0ff0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f16d3926bf7d42f3a864c8aaf75d8fa2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9acb681f98d74355b0d50fd3392c7ec2",
       "style": "IPY_MODEL_556f902dd03b49fbb6bf7120e444631e",
       "value": "Train: 100%"
      }
     },
     "f50ae3f29d31451f941bca0d4a03a215": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f8a513de577440318b252bef4f2da141": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
